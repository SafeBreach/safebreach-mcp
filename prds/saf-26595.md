# SAF-26595: Caching Mechanism Analysis - Memory Exhaustion Risk Assessment

## Executive Summary

This document provides a comprehensive analysis of all caching mechanisms implemented across the SafeBreach MCP Server project. The analysis identifies **critical memory exhaustion risks** due to unbounded cache growth patterns, lack of proper eviction policies, and architectural design issues that can lead to server instability in production environments.

**Risk Level: CRITICAL**

The current caching implementation will inevitably cause memory exhaustion in long-running production servers or under heavy AI agent usage patterns.

---

## Table of Contents

1. [Overview](#1-overview)
2. [Caching Architecture](#2-caching-architecture)
3. [Detailed Cache Analysis](#3-detailed-cache-analysis)
   - [Data Server Caches](#31-data-server-caches)
   - [Config Server Caches](#32-config-server-caches)
   - [Playbook Server Caches](#33-playbook-server-caches)
   - [Core Module Caches](#34-core-module-caches)
4. [Memory Growth Projections](#4-memory-growth-projections)
5. [Risk Assessment Matrix](#5-risk-assessment-matrix)
6. [Root Cause Analysis](#6-root-cause-analysis)
7. [Recommendations](#7-recommendations)
8. [Appendix](#8-appendix)

---

## 1. Overview

### 1.1 Purpose

The SafeBreach MCP Server implements caching to reduce API call latency and improve performance when serving AI agents. However, the current implementation lacks fundamental cache management features, leading to unbounded memory growth.

### 1.2 Scope

This analysis covers all caching mechanisms across:
- **Data Server** (`safebreach_mcp_data/`)
- **Config Server** (`safebreach_mcp_config/`)
- **Playbook Server** (`safebreach_mcp_playbook/`)
- **Utilities Server** (`safebreach_mcp_utilities/`) - No caching implemented
- **Core Modules** (`safebreach_mcp_core/`)

### 1.3 Key Findings

| Finding | Severity |
|---------|----------|
| No maximum cache size limits | CRITICAL |
| No LRU or eviction policy | CRITICAL |
| TTL only prevents stale reads, doesn't delete | HIGH |
| Cache keys with unbounded cardinality | CRITICAL |
| No memory monitoring or alerts | MEDIUM |
| No cache cleanup mechanism | HIGH |

---

## 2. Caching Architecture

### 2.1 Current Design Pattern

All caches in the project follow a similar pattern:

```python
# Module-level global dictionary
cache = {}
CACHE_TTL = 3600  # 1 hour

def get_data_from_cache_or_api(key):
    cache_key = f"prefix_{key}"
    current_time = time.time()

    # Check cache
    if cache_key in cache:
        data, timestamp = cache[cache_key]
        if current_time - timestamp < CACHE_TTL:
            return data  # Cache hit
        # ISSUE: Expired entry NOT deleted here

    # Fetch from API
    data = fetch_from_api()

    # Store in cache (always adds, never removes)
    cache[cache_key] = (data, current_time)
    return data
```

### 2.2 Architectural Issues

1. **Global Module-Level Dictionaries**: Caches persist for the entire server lifetime
2. **No Size Bounds**: Dictionaries grow indefinitely
3. **Passive TTL**: Expiration only checked on read, not actively enforced
4. **No Eviction**: Old entries never removed unless overwritten by same key
5. **Unbounded Key Space**: Some cache keys include IDs that grow indefinitely

---

## 3. Detailed Cache Analysis

### 3.1 Data Server Caches

The Data Server (`safebreach_mcp_data/data_functions.py`) contains the most complex and highest-risk caching implementations.

#### 3.1.1 Tests Cache

| Property | Value |
|----------|-------|
| **File** | `safebreach_mcp_data/data_functions.py` |
| **Line** | 27 |
| **Variable** | `tests_cache` |
| **Type** | `Dict[str, Tuple[List[Dict], float]]` |
| **Key Pattern** | `tests_{console}` |
| **TTL** | 3600 seconds (1 hour) |

**Implementation:**
```python
# Line 27
tests_cache = {}

# Line 194-202 - Cache key construction
cache_key = f"tests_{console}"

# Line 232 - Cache storage
tests_cache[cache_key] = (tests, current_time)
```

**Data Stored:**
- List of up to 1000 test summary objects per console
- Each test summary contains: `name`, `test_id`, `start_time`, `end_time`, `duration`, `status`, `test_type`

**Growth Pattern:**
- Bounded by number of consoles
- Each console entry can contain ~1000 tests
- Estimated size: ~500KB-2MB per console

**Risk Level:** LOW
- Key cardinality bounded by configured consoles
- Entries overwritten on cache refresh

---

#### 3.1.2 Simulations Cache (CRITICAL)

| Property | Value |
|----------|-------|
| **File** | `safebreach_mcp_data/data_functions.py` |
| **Line** | 28 |
| **Variable** | `simulations_cache` |
| **Type** | `Dict[str, Tuple[List[Dict], float]]` |
| **Key Pattern** | `simulations_{console}_{test_id}` |
| **TTL** | 3600 seconds (1 hour) |

**Implementation:**
```python
# Line 28
simulations_cache = {}

# Line 624 - Cache key construction
cache_key = f"simulations_{console}_{test_id}"

# Line 697 - Cache storage
simulations_cache[cache_key] = (simulations, current_time)
```

**Data Stored:**
- All simulations for a specific test (paginated fetch, 100/page)
- Each simulation contains: `simulation_id`, `test_name`, `test_id`, `start_time`, `end_time`, `status`, `playbook_attack_id`, `playbook_attack_name`, `drift_tracking_code`, `is_drifted`

**Growth Pattern:**
- **UNBOUNDED** - Key includes `test_id` which is unique per test execution
- New tests create new cache entries that are NEVER removed
- A test with 500 simulations creates ~250KB-1MB of cached data

**Risk Level:** CRITICAL

**Memory Growth Calculation:**
```
Consoles: 10
Tests per month per console: 50
Simulations per test: 500
Data per simulation: ~1KB

Monthly growth = 10 × 50 × 500 × 1KB = 250MB/month
Annual growth = 3GB/year (cumulative, never cleared)
```

---

#### 3.1.3 Security Control Events Cache (CRITICAL)

| Property | Value |
|----------|-------|
| **File** | `safebreach_mcp_data/data_functions.py` |
| **Line** | 29 |
| **Variable** | `security_control_events_cache` |
| **Type** | `Dict[str, Dict[str, Any]]` |
| **Key Pattern** | `{console}:{test_id}:{simulation_id}` |
| **TTL** | 3600 seconds (1 hour) |

**Implementation:**
```python
# Line 29
security_control_events_cache = {}

# Line 887 - Cache key construction
cache_key = f"{console}:{test_id}:{simulation_id}"

# Line 919-922 - Cache storage
security_control_events_cache[cache_key] = {
    'data': security_events,
    'timestamp': current_time
}
```

**Data Stored:**
- Raw SIEM log events for a specific simulation
- Variable size depending on security control activity
- Can include: `event_id`, `timestamp`, `vendor`, `product`, `action`, `source_hosts`, `destination_hosts`, raw log data

**Growth Pattern:**
- **UNBOUNDED** - Key includes both `test_id` AND `simulation_id`
- Every unique simulation queried creates a new cache entry
- SIEM events can be large (raw log data)

**Risk Level:** CRITICAL

**Memory Growth Calculation:**
```
Consoles: 10
Unique simulations queried per day: 100
SIEM events per simulation: 10-50
Data per event set: ~10KB-100KB

Daily growth (worst case) = 10 × 100 × 100KB = 100MB/day
Monthly growth = 3GB/month (if heavily used)
```

---

#### 3.1.4 Findings Cache (HIGH)

| Property | Value |
|----------|-------|
| **File** | `safebreach_mcp_data/data_functions.py` |
| **Line** | 1189 |
| **Variable** | `findings_cache` |
| **Type** | `Dict[str, Dict[str, Any]]` |
| **Key Pattern** | `{console}:{test_id}` |
| **TTL** | 3600 seconds (1 hour) |

**Implementation:**
```python
# Line 1189
findings_cache = {}

# Line 1206 - Cache key construction
cache_key = f"{console}:{test_id}"

# Line 1231-1234 - Cache storage
findings_cache[cache_key] = {
    'data': findings_data,
    'timestamp': current_time
}
```

**Data Stored:**
- Test findings from propagateSummary API
- Contains security findings similar to penetration test reports

**Growth Pattern:**
- **UNBOUNDED** - Key includes `test_id`
- Each unique test queried creates a new cache entry
- Findings data size varies based on test results

**Risk Level:** HIGH

---

#### 3.1.5 Full Simulation Logs Cache (CRITICAL)

| Property | Value |
|----------|-------|
| **File** | `safebreach_mcp_data/data_functions.py` |
| **Line** | 1639 |
| **Variable** | `full_simulation_logs_cache` |
| **Type** | `Dict[str, Tuple[Dict, float]]` |
| **Key Pattern** | `full_simulation_logs_{console}_{simulation_id}_{test_id}` |
| **TTL** | 3600 seconds (1 hour) |

**Implementation:**
```python
# Line 1639
full_simulation_logs_cache = {}

# Line 1727 - Cache key construction
cache_key = f"full_simulation_logs_{console}_{simulation_id}_{test_id}"

# Line 1743 - Cache storage
full_simulation_logs_cache[cache_key] = (data, current_time)
```

**Data Stored:**
- Comprehensive execution logs (~40KB per simulation as documented)
- Includes: detailed LOGS field, simulation_steps, details_summary, output, metadata

**Growth Pattern:**
- **UNBOUNDED** - Key includes `simulation_id` and `test_id`
- Each log query caches **~40KB** of data
- Most dangerous cache due to per-entry size

**Risk Level:** CRITICAL

**Memory Growth Calculation:**
```
Log queries per day: 100 simulations
Data per entry: 40KB

Daily growth = 100 × 40KB = 4MB/day
Monthly growth = 120MB/month
Annual growth = 1.4GB/year (for modest usage)

Heavy usage (1000 queries/day) = 40MB/day = 1.2GB/month
```

---

### 3.2 Config Server Caches

#### 3.2.1 Simulators Cache

| Property | Value |
|----------|-------|
| **File** | `safebreach_mcp_config/config_functions.py` |
| **Line** | 19 |
| **Variable** | `simulators_cache` |
| **Type** | `Dict[str, Tuple[List[Dict], float]]` |
| **Key Pattern** | `simulators_{console}` |
| **TTL** | 3600 seconds (1 hour) |

**Implementation:**
```python
# Line 19
simulators_cache = {}

# Line 130 - Cache key construction
cache_key = f"simulators_{console}"

# Line 169 - Cache storage
simulators_cache[cache_key] = (simulators, current_time)
```

**Data Stored:**
- All simulators for a console
- Each simulator contains: `id`, `name`, `version`, `isConnected`, `isEnabled`, `labels`, `OS`, `isCritical`

**Growth Pattern:**
- Bounded by number of consoles
- Each console typically has 10-100 simulators
- Estimated size: ~50KB-500KB per console

**Risk Level:** LOW
- Key cardinality bounded by configured consoles

---

### 3.3 Playbook Server Caches

#### 3.3.1 Playbook Cache

| Property | Value |
|----------|-------|
| **File** | `safebreach_mcp_playbook/playbook_functions.py` |
| **Line** | 25 |
| **Variable** | `playbook_cache` |
| **Type** | `Dict[str, Dict[str, Any]]` |
| **Key Pattern** | `attacks_{console}` |
| **TTL** | 3600 seconds (1 hour) |

**Implementation:**
```python
# Line 25
playbook_cache = {}

# Line 47 - Cache key construction
cache_key = f"attacks_{console}"

# Line 84-87 - Cache storage
playbook_cache[cache_key] = {
    'data': attacks_data,
    'timestamp': current_time
}
```

**Data Stored:**
- All playbook attacks for a console
- SafeBreach attack knowledge base (thousands of attacks)
- Each attack contains: `id`, `name`, `description`, dates, tags, parameters

**Growth Pattern:**
- Bounded by number of consoles
- Large per-entry size (thousands of attacks × ~1KB each = ~5-10MB per console)
- But bounded key cardinality

**Risk Level:** MEDIUM
- Large per-entry size but bounded growth
- A server with 10 consoles could cache 50-100MB of playbook data

---

### 3.4 Core Module Caches

#### 3.4.1 Base Server Cache

| Property | Value |
|----------|-------|
| **File** | `safebreach_mcp_core/safebreach_base.py` |
| **Line** | 54-55 |
| **Variable** | `_cache`, `_cache_timestamps` |
| **Type** | `Dict[str, Any]`, `Dict[str, float]` |
| **Key Pattern** | User-defined |
| **TTL** | 3600 seconds (1 hour) |

**Implementation:**
```python
# Lines 54-55
self._cache = {}
self._cache_timestamps = {}

# Lines 60-78 - Cache access with TTL and deletion
def get_from_cache(self, key: str) -> Optional[Dict[str, Any]]:
    if key in self._cache:
        timestamp = self._cache_timestamps.get(key, 0)
        if time.time() - timestamp < self.CACHE_TTL:
            return self._cache[key]
        else:
            # Remove expired entry - ONLY CACHE WITH DELETION
            del self._cache[key]
            del self._cache_timestamps[key]
    return None
```

**Note:** This is the ONLY cache implementation that actually deletes expired entries. However, this base class cache is not used by the function-level caches in data_functions.py, config_functions.py, or playbook_functions.py.

**Risk Level:** LOW (if used)
- Has proper TTL-based deletion
- Per-server-instance isolation

---

#### 3.4.2 SafeBreachAuth Token Cache

| Property | Value |
|----------|-------|
| **File** | `safebreach_mcp_core/safebreach_auth.py` |
| **Line** | 17 |
| **Variable** | `_token_cache` |
| **Type** | `Dict[str, str]` |
| **Key Pattern** | `{console}` |
| **TTL** | None (permanent) |

**Implementation:**
```python
# Line 17
self._token_cache = {}

# Lines 21-25 - Cache access (no TTL)
def get_token(self, console: str) -> str:
    if console not in self._token_cache:
        self._token_cache[console] = get_secret_for_console(console)
    return self._token_cache[console]
```

**Growth Pattern:**
- Bounded by number of consoles
- Small per-entry size (API tokens ~100 bytes)

**Risk Level:** LOW
- Bounded key cardinality
- Small data size
- No TTL but tokens are typically long-lived

---

#### 3.4.3 Secret Provider Cache

| Property | Value |
|----------|-------|
| **File** | `safebreach_mcp_core/secret_utils.py` |
| **Line** | 17 |
| **Variable** | `_provider_cache` |
| **Type** | `Dict[str, SecretProvider]` |
| **Key Pattern** | `{provider_type}:{region}` |
| **TTL** | None (permanent) |

**Implementation:**
```python
# Line 17
_provider_cache: Dict[str, SecretProvider] = {}

# Lines 75-76 - Cache key construction
cache_key = f"{provider_type}:{secret_config.get('region_name', 'us-east-1')}"
```

**Growth Pattern:**
- Bounded by provider types (3: aws_ssm, aws_secrets_manager, env_var) × regions

**Risk Level:** LOW
- Very limited key cardinality

---

#### 3.4.4 Individual Secret Provider Caches

| Property | Value |
|----------|-------|
| **File** | `safebreach_mcp_core/secret_providers.py` |
| **Lines** | 50, 103, 150 |
| **Variables** | `_cache` (per provider instance) |
| **Type** | `Dict[str, str]` |
| **Key Pattern** | Secret identifier |
| **TTL** | None (permanent) |

**Implementations:**

**AWSSSMSecretProvider (Line 50):**
```python
self._cache: Dict[str, str] = {}
```

**AWSSecretsManagerProvider (Line 103):**
```python
self._cache: Dict[str, str] = {}
```

**EnvVarSecretProvider (Line 150):**
```python
self._cache: Dict[str, str] = {}
```

**Growth Pattern:**
- Bounded by number of secrets (typically 1 per console)
- Small per-entry size

**Risk Level:** LOW

---

## 4. Memory Growth Projections

### 4.1 Baseline Assumptions

| Parameter | Conservative | Moderate | Heavy Usage |
|-----------|--------------|----------|-------------|
| Consoles | 5 | 10 | 20 |
| Tests/month/console | 20 | 50 | 100 |
| Simulations/test | 200 | 500 | 1000 |
| Log queries/day | 10 | 100 | 500 |
| Event queries/day | 20 | 100 | 500 |

### 4.2 Monthly Memory Growth

| Cache | Conservative | Moderate | Heavy Usage |
|-------|--------------|----------|-------------|
| `simulations_cache` | 20MB | 250MB | 2GB |
| `security_control_events_cache` | 10MB | 100MB | 1.5GB |
| `full_simulation_logs_cache` | 12MB | 120MB | 600MB |
| `findings_cache` | 5MB | 50MB | 200MB |
| **Total Monthly Growth** | **47MB** | **520MB** | **4.3GB** |

### 4.3 Long-Term Projections (Without Restart)

| Timeline | Moderate Usage | Heavy Usage |
|----------|---------------|-------------|
| 1 month | 520MB | 4.3GB |
| 3 months | 1.5GB | 13GB |
| 6 months | 3.1GB | 26GB |
| 12 months | 6.2GB | 52GB |

**Note:** These are cumulative values assuming no server restarts and no cache cleanup.

---

## 5. Risk Assessment Matrix

| Cache | Location | Key Cardinality | Entry Size | TTL Eviction | Risk Level |
|-------|----------|-----------------|------------|--------------|------------|
| `tests_cache` | data_functions.py:27 | Bounded (consoles) | Medium | No | LOW |
| `simulations_cache` | data_functions.py:28 | **Unbounded** | Large | No | **CRITICAL** |
| `security_control_events_cache` | data_functions.py:29 | **Unbounded** | Variable/Large | No | **CRITICAL** |
| `findings_cache` | data_functions.py:1189 | **Unbounded** | Medium | No | HIGH |
| `full_simulation_logs_cache` | data_functions.py:1639 | **Unbounded** | **~40KB each** | No | **CRITICAL** |
| `simulators_cache` | config_functions.py:19 | Bounded (consoles) | Medium | No | LOW |
| `playbook_cache` | playbook_functions.py:25 | Bounded (consoles) | Large | No | MEDIUM |
| `_cache` (base) | safebreach_base.py:54 | User-defined | Variable | **Yes** | LOW |
| `_token_cache` | safebreach_auth.py:17 | Bounded (consoles) | Small | No | LOW |
| `_provider_cache` | secret_utils.py:17 | Bounded (~3×regions) | Small | No | LOW |
| Provider `_cache` | secret_providers.py | Bounded (secrets) | Small | No | LOW |

---

## 6. Root Cause Analysis

### 6.1 Design Flaws

#### 6.1.1 TTL Implementation Does Not Evict

The TTL mechanism only prevents reading stale data; it does not remove entries:

```python
# Current implementation (data_functions.py)
if cache_key in tests_cache:
    data, timestamp = tests_cache[cache_key]
    if current_time - timestamp < CACHE_TTL:
        return data  # Return if fresh
    # PROBLEM: Falls through to fetch, but old entry remains!

# What happens:
# 1. Entry expires (timestamp > TTL)
# 2. New data fetched from API
# 3. New entry ADDED with same key (overwrites)
# 4. BUT: Different keys are NEVER removed
```

**Contrast with safebreach_base.py which does delete:**
```python
else:
    # Remove expired entry
    del self._cache[key]
    del self._cache_timestamps[key]
```

#### 6.1.2 Cache Keys Include Unbounded Identifiers

```python
# PROBLEM: test_id is unique per test execution
cache_key = f"simulations_{console}_{test_id}"

# PROBLEM: simulation_id is unique per simulation
cache_key = f"{console}:{test_id}:{simulation_id}"

# PROBLEM: Both test_id and simulation_id
cache_key = f"full_simulation_logs_{console}_{simulation_id}_{test_id}"
```

#### 6.1.3 No Maximum Cache Size

```python
# No size check before adding
simulations_cache[cache_key] = (simulations, current_time)
```

### 6.2 Missing Features

| Feature | Status | Impact |
|---------|--------|--------|
| Maximum cache size | Missing | Unbounded growth |
| LRU eviction | Missing | Old entries never removed |
| Memory monitoring | Missing | No visibility into usage |
| Cache statistics | Missing | Cannot track hit/miss rates |
| Periodic cleanup | Missing | No background maintenance |
| Per-entry size limits | Missing | Large entries cached |

---

## 7. Recommendations

### 7.1 Short-Term Mitigations

#### 7.1.1 Implement Bounded Caches

Replace unbounded dictionaries with size-limited caches:

```python
from collections import OrderedDict

class BoundedCache:
    def __init__(self, max_size: int = 1000, ttl: int = 3600):
        self._cache = OrderedDict()
        self._timestamps = {}
        self._max_size = max_size
        self._ttl = ttl

    def get(self, key: str):
        if key not in self._cache:
            return None
        if time.time() - self._timestamps[key] > self._ttl:
            self._evict(key)
            return None
        self._cache.move_to_end(key)  # LRU: move to end on access
        return self._cache[key]

    def set(self, key: str, value):
        if key in self._cache:
            self._cache.move_to_end(key)
        else:
            if len(self._cache) >= self._max_size:
                self._evict_oldest()
        self._cache[key] = value
        self._timestamps[key] = time.time()

    def _evict(self, key: str):
        del self._cache[key]
        del self._timestamps[key]

    def _evict_oldest(self):
        oldest_key = next(iter(self._cache))
        self._evict(oldest_key)
```

#### 7.1.2 Add Active TTL Eviction

Implement periodic cleanup of expired entries:

```python
import threading

def start_cache_cleanup_thread(interval: int = 300):  # Every 5 minutes
    def cleanup():
        while True:
            time.sleep(interval)
            current_time = time.time()

            # Clean each cache
            for cache, timestamps in [
                (simulations_cache, None),
                (security_control_events_cache, None),
                # ... other caches
            ]:
                keys_to_delete = []
                for key, (data, timestamp) in cache.items():
                    if current_time - timestamp > CACHE_TTL:
                        keys_to_delete.append(key)

                for key in keys_to_delete:
                    del cache[key]

    thread = threading.Thread(target=cleanup, daemon=True)
    thread.start()
```

### 7.2 Medium-Term Improvements

#### 7.2.1 Use Python's functools.lru_cache

For simple cases:

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def get_simulations_cached(console: str, test_id: str):
    return fetch_simulations_from_api(console, test_id)
```

#### 7.2.2 Implement Memory-Aware Caching

```python
import sys

class MemoryAwareCache:
    def __init__(self, max_memory_mb: int = 256):
        self._cache = {}
        self._max_memory = max_memory_mb * 1024 * 1024

    def _get_cache_size(self):
        return sum(sys.getsizeof(v) for v in self._cache.values())

    def set(self, key, value):
        # Evict until under memory limit
        while self._get_cache_size() > self._max_memory and self._cache:
            oldest = next(iter(self._cache))
            del self._cache[oldest]
        self._cache[key] = value
```

### 7.3 Long-Term Architecture

#### 7.3.1 Consider External Cache

For production deployments, consider Redis or Memcached:

```python
import redis

class RedisCache:
    def __init__(self, host='localhost', port=6379, ttl=3600):
        self._client = redis.Redis(host=host, port=port)
        self._ttl = ttl

    def get(self, key: str):
        value = self._client.get(key)
        return json.loads(value) if value else None

    def set(self, key: str, value):
        self._client.setex(key, self._ttl, json.dumps(value))
```

Benefits:
- Automatic TTL expiration
- Memory limits enforced by Redis
- Shared cache across server instances
- Persistence options

#### 7.3.2 Redesign Cache Keys

For caches that must include test_id or simulation_id, implement a hierarchical approach:

```python
# Instead of: simulations_cache[f"{console}_{test_id}"]
# Use a two-level structure:

simulations_cache = {
    "console1": {
        "test_123": (data, timestamp),
        "test_456": (data, timestamp),
    }
}

# With per-console limits
MAX_TESTS_PER_CONSOLE = 100
```

### 7.4 Recommended Cache Limits

| Cache | Recommended Max Size | Rationale |
|-------|---------------------|-----------|
| `tests_cache` | 50 entries | ~1 per console expected |
| `simulations_cache` | 500 entries | Most recent tests only |
| `security_control_events_cache` | 200 entries | Recent queries only |
| `findings_cache` | 200 entries | Recent tests only |
| `full_simulation_logs_cache` | 100 entries | Large entries (~40KB) |
| `simulators_cache` | 50 entries | ~1 per console expected |
| `playbook_cache` | 50 entries | ~1 per console expected |

---

## 8. Appendix

### 8.1 Cache Location Summary

| File | Line | Variable | Risk |
|------|------|----------|------|
| `safebreach_mcp_data/data_functions.py` | 27 | `tests_cache` | LOW |
| `safebreach_mcp_data/data_functions.py` | 28 | `simulations_cache` | CRITICAL |
| `safebreach_mcp_data/data_functions.py` | 29 | `security_control_events_cache` | CRITICAL |
| `safebreach_mcp_data/data_functions.py` | 1189 | `findings_cache` | HIGH |
| `safebreach_mcp_data/data_functions.py` | 1639 | `full_simulation_logs_cache` | CRITICAL |
| `safebreach_mcp_config/config_functions.py` | 19 | `simulators_cache` | LOW |
| `safebreach_mcp_playbook/playbook_functions.py` | 25 | `playbook_cache` | MEDIUM |
| `safebreach_mcp_core/safebreach_base.py` | 54-55 | `_cache`, `_cache_timestamps` | LOW |
| `safebreach_mcp_core/safebreach_auth.py` | 17 | `_token_cache` | LOW |
| `safebreach_mcp_core/secret_utils.py` | 17 | `_provider_cache` | LOW |
| `safebreach_mcp_core/secret_providers.py` | 50 | `AWSSSMSecretProvider._cache` | LOW |
| `safebreach_mcp_core/secret_providers.py` | 103 | `AWSSecretsManagerProvider._cache` | LOW |
| `safebreach_mcp_core/secret_providers.py` | 150 | `EnvVarSecretProvider._cache` | LOW |

### 8.2 Utilities Server Note

The Utilities Server (`safebreach_mcp_utilities/utilities_server.py`) does **not** implement any caching. It provides stateless datetime conversion functions:
- `convert_datetime_to_epoch`
- `convert_epoch_to_datetime`

No caching analysis required for this server.

### 8.3 Test Coverage Considerations

When implementing fixes, ensure test coverage for:
- Cache size limit enforcement
- LRU eviction behavior
- TTL expiration and deletion
- Memory limit enforcement
- Concurrent access safety
- Cache statistics accuracy

### 8.4 Related Documentation

- `CLAUDE.md` - Project development guidelines
- `README.md` - Architecture overview and caching strategy section
- `DESIGN.md` - Design documentation (if applicable)

---

## Document Metadata

| Field | Value |
|-------|-------|
| **Ticket** | SAF-26595 |
| **Author** | Claude Code Analysis |
| **Date** | 2025-12-21 |
| **Version** | 1.0 |
| **Status** | Analysis Complete |
| **Branch** | `bug/optimize-caching` |
